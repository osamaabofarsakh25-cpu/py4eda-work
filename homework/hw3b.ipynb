{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# HW3B - Pandas Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "See Canvas for details on how to complete and submit this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "This assignment transitions you from NumPy's numerical array operations to Pandas' powerful tabular data manipulation. While NumPy excels at homogeneous numerical arrays, Pandas is designed for the heterogeneous, labeled data that characterizes most real-world datasets—mixing dates, categories, numbers, and text within the same table.\n",
    "\n",
    "You'll work with real bike share data from Chicago's Divvy system to answer questions about urban transportation patterns. Through three progressively complex problems—exploring usage patterns, analyzing rider behavior, and conducting temporal analysis—you'll discover why Pandas has become the standard tool for data analysis in Python.\n",
    "\n",
    "The assignment emphasizes Pandas' design philosophy: named column access, explicit indexing methods (loc/iloc), handling missing data, and method chaining for readable data pipelines. You'll also see how Pandas builds on NumPy while adding the structure and convenience needed for practical data science work.\n",
    "\n",
    "This assignment should take 3-5 hours to complete.\n",
    "\n",
    "Before submitting, ensure your notebook:\n",
    "\n",
    "- Runs completely with \"Kernel → Restart & Run All\"\n",
    "- Includes thoughtful responses to all interpretation questions\n",
    "- Uses clear variable names and follows good coding practices\n",
    "- Shows your work (don't just print final answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "By completing this assignment, you will be able to:\n",
    "\n",
    "1. **Construct and manipulate Pandas data structures**\n",
    "   - Create DataFrames from dictionaries and CSV files\n",
    "   - Distinguish between Series and DataFrame objects\n",
    "   - Set and reset index structures appropriately\n",
    "   - Understand when operations return views vs copies\n",
    "2. **Apply explicit indexing paradigms**\n",
    "   - Use `loc[]` for label-based data access\n",
    "   - Use `iloc[]` for position-based data access\n",
    "   - Access columns using bracket notation\n",
    "   - Explain when each indexing method is appropriate\n",
    "3. **Diagnose and explore datasets systematically**\n",
    "   - Use `info()`, `describe()`, `head()`, and `dtypes` to understand data structure\n",
    "   - Identify missing values with `isna()` and `notna()`\n",
    "   - Calculate summary statistics across different axes\n",
    "   - Interpret value distributions with `value_counts()`\n",
    "4. **Filter data with boolean indexing and queries**\n",
    "   - Combine multiple conditions with `&`, `|`, and `~` operators\n",
    "   - Use `isin()` for membership testing\n",
    "   - Apply `query()` for readable complex filters\n",
    "   - Understand how index alignment affects operations\n",
    "5. **Work with datetime data**\n",
    "   - Parse dates during CSV loading\n",
    "   - Extract temporal components with the `.dt` accessor\n",
    "   - Filter data by date ranges\n",
    "   - Create time-based derived features\n",
    "6. **Connect Pandas patterns to data analysis workflows**\n",
    "   - Formulate questions that data can answer\n",
    "   - Choose appropriate methods for different analysis tasks\n",
    "   - Interpret results in domain context\n",
    "   - Recognize when vectorized operations outperform apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Generative AI Allowance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "You may use GenAI tools for brainstorming, explanations, and code sketches if you disclose it, understand it, and validate it. Your submission must represent your own work and you are solely responsible for its correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Total of 90 points available, will be graded out of 80. Scores of >100% are allowed.\n",
    "\n",
    "Distribution:\n",
    "\n",
    "- Tasks: 48 pts\n",
    "- Interpretation: 32 pts\n",
    "- Reflection: 10 pts\n",
    "\n",
    "Points by Problem:\n",
    "\n",
    "- Problem 1: 3 tasks, 10 pts\n",
    "- Problem 2: 4 tasks, 14 pts\n",
    "- Problem 3: 4 tasks, 14 pts\n",
    "- Problem 4: 3 tasks, 10 pts\n",
    "\n",
    "Interpretation Questions:\n",
    "\n",
    "- Problem 1: 3 questions, 8 pts\n",
    "- Problem 2: 4 questions, 8 pts\n",
    "- Problem 3: 3 questions, 8 pts\n",
    "- Problem 4: 3 questions, 8 pts\n",
    "\n",
    "Graduate differentiation: poor follow-up responses will result in up to a 5pt deduction for that problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Dataset: Chicago Divvy Bike Share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The dataset you will analyze is based on real trip information from Divvy, Chicago's bike share system. It contains individual trips with start/end times, station information, and rider type.\n",
    "\n",
    "Dataset homepage: https://divvybikes.com/system-data\n",
    "\n",
    "Each trip includes:\n",
    "\n",
    "- Trip start and end times (datetime)\n",
    "- Start and end station names and IDs\n",
    "- Rider type (member vs casual)\n",
    "- Bike type (classic, electric, or docked)\n",
    "\n",
    "Chicago's Department of Transportation uses this data to optimize station placement, understand usage patterns, and improve service. You'll explore similar questions that real transportation analysts investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Problem 1: Creating DataFrames from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Before loading data from files, you need to understand how Pandas structures are built. In this problem, you'll create Series and DataFrames manually using Python's built-in data structures. This is a quick warmup to establish the fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Task 1a: Create a Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Create a Series called `temperatures` representing daily high temperatures for a week:\n",
    "\n",
    "- Monday: 72°F\n",
    "- Tuesday: 75°F  \n",
    "- Wednesday: 68°F\n",
    "- Thursday: 71°F\n",
    "- Friday: 73°F\n",
    "\n",
    "Use the day names as the index. Print the Series and its data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monday       72\n",
      "Tuesday      75\n",
      "Wednesday    68\n",
      "Thursday     71\n",
      "Friday       73\n",
      "dtype: int64\n",
      "\n",
      "Data type:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create temperature series \n",
    "temperatures = pd.Series( \n",
    "    [72, 75, 68, 71, 73],\n",
    "    index=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
    ")\n",
    "\n",
    "# printing results \n",
    "print(temperatures) \n",
    "print (\"\\nData type: \", type(temperatures))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Task 1b: Create a DataFrame from a Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Create a DataFrame called `products` with the following data:\n",
    "\n",
    "| product | price | quantity |\n",
    "|---------|-------|----------|\n",
    "| Widget  | 19.99 | 100 |\n",
    "| Gadget  | 24.99 | 75 |\n",
    "| Doohickey | 12.49 | 150 |\n",
    "\n",
    "Use a dictionary where keys are column names and values are lists. Print the DataFrame and report its shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column types:\n",
      " product      object\n",
      "price       float64\n",
      "quantity      int64\n",
      "dtype: object\n",
      "\n",
      "Shape: (3, 3)\n",
      "\n",
      "Products DataFrame:\n",
      "      product  price  quantity\n",
      "0     Widget  19.99       100\n",
      "1     Gadget  24.99        75\n",
      "2  Doohickey  12.49       150 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating the DataFrame\n",
    "products= pd.DataFrame({\n",
    "    \"product\": ['Widget', 'Gadget', 'Doohickey'],\n",
    "    \"price\": [19.99, 24.99, 12.49],\n",
    "    \"quantity\":[100, 75, 150]\n",
    "})\n",
    "\n",
    "# Show the data types of each column\n",
    "print(\"Column types:\\n\", products.dtypes)\n",
    "\n",
    "# Show shape \n",
    "print(\"\\nShape:\", products.shape) \n",
    "\n",
    "# Display DataFrame\n",
    "print(\"\\nProducts DataFrame:\\n\", products, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Task 1c: Access DataFrame Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Using the `products` DataFrame from Task 1b, extract and print:\n",
    "\n",
    "1. The `price` column as a Series\n",
    "2. The `product` and `quantity` columns as a DataFrame (using a list of column names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price column as Series:\n",
      " 0    19.99\n",
      "1    24.99\n",
      "2    12.49\n",
      "Name: price, dtype: float64 \n",
      "\n",
      "\n",
      "price_series type <class 'pandas.core.series.Series'>\n",
      "\n",
      " 1d-series: (3,)\n",
      "\n",
      "Product and Quantity columns DataFrame:\n",
      "      product  quantity\n",
      "0     Widget       100\n",
      "1     Gadget        75\n",
      "2  Doohickey       150\n",
      "\n",
      "Columns_DataFrame type <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      " 2d-shape: (3, 2)\n"
     ]
    }
   ],
   "source": [
    "# Extract price column as a Series\n",
    "price_series = products['price']\n",
    "print(\"Price column as Series:\\n\", price_series, \"\\n\")\n",
    "print(\"\\nprice_series type\", type(price_series))\n",
    "print(\"\\n 1d-series:\", price_series.shape)\n",
    "# Using a list of column names\n",
    "Columns_DataFrame = products[['product', 'quantity']]\n",
    "print(\"\\nProduct and Quantity columns DataFrame:\\n\", Columns_DataFrame)\n",
    "print(\"\\nColumns_DataFrame type\", type(Columns_DataFrame))\n",
    "print(\"\\n 2d-shape:\", Columns_DataFrame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Answer the following questions (briefly / concisely) in the markdown cell below:\n",
    "\n",
    "1. Data structure mapping: When you create a DataFrame from a dictionary (like in Task 1b), what do the dictionary keys become? What do the values become?\n",
    "2. Bracket notation: Why does `df['price']` return a Series, but `df[['price']]` return a DataFrame? What's the difference in what you're asking for?\n",
    "3. Index purpose: In Task 1a, you used day names as the index instead of default numbers (0, 1, 2...). When would a custom index like this be more useful than the default numeric index?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "***\n",
    "##### Data structure mapping\n",
    "A DataFrame constructed from a dictionary uses the dictionary's keys for column names, and the dictionary's values (in list or array form) are the values inside those columns.\n",
    "\n",
    "##### Bracket notation\n",
    "df['price'] returns a Series because a single column got selected, unlike when df[['price']] returns a DataFrame because a column list got selected. Single brackets signify 1D Series. Double brackets signify a 2D DataFrame.\n",
    "\n",
    "##### Index purpose\n",
    "A custom index (for example, day names) is more useful when it has meaningful labels that make the data easier to interpret or retrieve, for example, using dates, categories, or identifiers instead of generic row numbers.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Problem 2: Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Before starting this problem, make sure you are working in a copy of this file in the `my_repo` folder you created in HW2a. You must also have a copy of the file `202410-divvy-tripdata-100k.csv` in a subdirectory called `data`. That file structure is illustrated below.\n",
    "\n",
    "```text\n",
    "~/insy6500/my_repo\n",
    "└── homework\n",
    "    ├── data\n",
    "    │   └── 202410-divvy-tripdata-100k.csv\n",
    "    └── hw3b.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Task 2a: Load and Understand Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Start by loading the data \"as-is\" to get a general understanding of the overall structure and how Pandas interprets it by default.\n",
    "\n",
    "Note on file paths: The provided code uses `Path` from Python's `pathlib` module to handle file paths. Path objects work consistently across operating systems (Windows uses backslashes `\\`, Mac/Linux use forward slashes `/`), automatically using the correct separator for your system. The provided code defines `csv_path` which should be used as the filename in your `pd.read_csv` to load the data file.\n",
    "\n",
    "1. Use `pd.read_csv` to load `csv_path` (provided below) without specifying any other arguments. Assign it to the variable `df_raw`.\n",
    "2. Use the methods we described in class to explore the shape, structure, types, etc. of the data. In particular, consider which columns represent dates or categories.\n",
    "3. Note the amount of memory used by the dataset. See the section on memory diagnostics in notebook 07a for appropriate code snippets using `memory_usage`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (100000, 13)\n",
      "\n",
      "---------dtype---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ride_id                object\n",
       "rideable_type          object\n",
       "started_at             object\n",
       "ended_at               object\n",
       "start_station_name     object\n",
       "start_station_id       object\n",
       "end_station_name       object\n",
       "end_station_id         object\n",
       "start_lat             float64\n",
       "start_lng             float64\n",
       "end_lat               float64\n",
       "end_lng               float64\n",
       "member_casual          object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- df_raw.info() ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   ride_id             100000 non-null  object \n",
      " 1   rideable_type       100000 non-null  object \n",
      " 2   started_at          100000 non-null  object \n",
      " 3   ended_at            100000 non-null  object \n",
      " 4   start_station_name  89623 non-null   object \n",
      " 5   start_station_id    89623 non-null   object \n",
      " 6   end_station_name    89485 non-null   object \n",
      " 7   end_station_id      89485 non-null   object \n",
      " 8   start_lat           100000 non-null  float64\n",
      " 9   start_lng           100000 non-null  float64\n",
      " 10  end_lat             99913 non-null   float64\n",
      " 11  end_lng             99913 non-null   float64\n",
      " 12  member_casual       100000 non-null  object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 9.9+ MB\n",
      "\n",
      "-----------Quick statistical information--------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>99913.000000</td>\n",
       "      <td>99913.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>41.898817</td>\n",
       "      <td>-87.644839</td>\n",
       "      <td>41.899246</td>\n",
       "      <td>-87.645279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.045897</td>\n",
       "      <td>0.027118</td>\n",
       "      <td>0.046581</td>\n",
       "      <td>0.028055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>41.648501</td>\n",
       "      <td>-87.840000</td>\n",
       "      <td>41.610000</td>\n",
       "      <td>-89.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>41.879356</td>\n",
       "      <td>-87.658902</td>\n",
       "      <td>41.879472</td>\n",
       "      <td>-87.659172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>41.894666</td>\n",
       "      <td>-87.641180</td>\n",
       "      <td>41.894822</td>\n",
       "      <td>-87.641255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>41.925566</td>\n",
       "      <td>-87.627716</td>\n",
       "      <td>41.925858</td>\n",
       "      <td>-87.628579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42.070000</td>\n",
       "      <td>-87.530000</td>\n",
       "      <td>43.930000</td>\n",
       "      <td>-86.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           start_lat      start_lng       end_lat       end_lng\n",
       "count  100000.000000  100000.000000  99913.000000  99913.000000\n",
       "mean       41.898817     -87.644839     41.899246    -87.645279\n",
       "std         0.045897       0.027118      0.046581      0.028055\n",
       "min        41.648501     -87.840000     41.610000    -89.120000\n",
       "25%        41.879356     -87.658902     41.879472    -87.659172\n",
       "50%        41.894666     -87.641180     41.894822    -87.641255\n",
       "75%        41.925566     -87.627716     41.925858    -87.628579\n",
       "max        42.070000     -87.530000     43.930000    -86.050000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------First 5 rows ----------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67BB74BD7667BAB7</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-09-30 23:12:01.622</td>\n",
       "      <td>2024-10-01 00:20:00.674</td>\n",
       "      <td>Oakley Ave &amp; Touhy Ave</td>\n",
       "      <td>bdd4c3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.012342</td>\n",
       "      <td>-87.688243</td>\n",
       "      <td>41.970000</td>\n",
       "      <td>-87.650000</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5AF1AC3BA86ED58C</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-09-30 23:19:25.409</td>\n",
       "      <td>2024-10-01 00:42:09.933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Benson Ave &amp; Church St</td>\n",
       "      <td>a10cf0</td>\n",
       "      <td>42.070000</td>\n",
       "      <td>-87.730000</td>\n",
       "      <td>42.048214</td>\n",
       "      <td>-87.683485</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7961DD2FC1280CDC</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-09-30 23:32:24.672</td>\n",
       "      <td>2024-10-01 00:23:18.647</td>\n",
       "      <td>St. Clair St &amp; Erie St</td>\n",
       "      <td>9c619a</td>\n",
       "      <td>LaSalle St &amp; Illinois St</td>\n",
       "      <td>fbd1ad</td>\n",
       "      <td>41.894345</td>\n",
       "      <td>-87.622798</td>\n",
       "      <td>41.890762</td>\n",
       "      <td>-87.631697</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2E16892DEEF4CC19</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-09-30 23:42:11.207</td>\n",
       "      <td>2024-10-01 00:10:16.831</td>\n",
       "      <td>Ashland Ave &amp; Chicago Ave</td>\n",
       "      <td>72a04d</td>\n",
       "      <td>Loomis St &amp; Archer Ave</td>\n",
       "      <td>896337</td>\n",
       "      <td>41.895954</td>\n",
       "      <td>-87.667728</td>\n",
       "      <td>41.841633</td>\n",
       "      <td>-87.657435</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAF0220F819BEE01</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-09-30 23:49:25.380</td>\n",
       "      <td>2024-10-01 00:06:27.476</td>\n",
       "      <td>900 W Harrison St</td>\n",
       "      <td>11da85</td>\n",
       "      <td>900 W Harrison St</td>\n",
       "      <td>11da85</td>\n",
       "      <td>41.874754</td>\n",
       "      <td>-87.649807</td>\n",
       "      <td>41.874754</td>\n",
       "      <td>-87.649807</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type               started_at  \\\n",
       "0  67BB74BD7667BAB7  electric_bike  2024-09-30 23:12:01.622   \n",
       "1  5AF1AC3BA86ED58C  electric_bike  2024-09-30 23:19:25.409   \n",
       "2  7961DD2FC1280CDC   classic_bike  2024-09-30 23:32:24.672   \n",
       "3  2E16892DEEF4CC19   classic_bike  2024-09-30 23:42:11.207   \n",
       "4  AAF0220F819BEE01  electric_bike  2024-09-30 23:49:25.380   \n",
       "\n",
       "                  ended_at         start_station_name start_station_id  \\\n",
       "0  2024-10-01 00:20:00.674     Oakley Ave & Touhy Ave           bdd4c3   \n",
       "1  2024-10-01 00:42:09.933                        NaN              NaN   \n",
       "2  2024-10-01 00:23:18.647     St. Clair St & Erie St           9c619a   \n",
       "3  2024-10-01 00:10:16.831  Ashland Ave & Chicago Ave           72a04d   \n",
       "4  2024-10-01 00:06:27.476          900 W Harrison St           11da85   \n",
       "\n",
       "           end_station_name end_station_id  start_lat  start_lng    end_lat  \\\n",
       "0                       NaN            NaN  42.012342 -87.688243  41.970000   \n",
       "1    Benson Ave & Church St         a10cf0  42.070000 -87.730000  42.048214   \n",
       "2  LaSalle St & Illinois St         fbd1ad  41.894345 -87.622798  41.890762   \n",
       "3    Loomis St & Archer Ave         896337  41.895954 -87.667728  41.841633   \n",
       "4         900 W Harrison St         11da85  41.874754 -87.649807  41.874754   \n",
       "\n",
       "     end_lng member_casual  \n",
       "0 -87.650000        casual  \n",
       "1 -87.683485        casual  \n",
       "2 -87.631697        member  \n",
       "3 -87.657435        casual  \n",
       "4 -87.649807        member  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory usage per column:\n",
      "Index                     132\n",
      "ride_id               6500000\n",
      "rideable_type         6150493\n",
      "started_at            7200000\n",
      "ended_at              7200000\n",
      "start_station_name    6832669\n",
      "start_station_id      5261329\n",
      "end_station_name      6830739\n",
      "end_station_id        5258155\n",
      "start_lat              800000\n",
      "start_lng              800000\n",
      "end_lat                800000\n",
      "end_lng                800000\n",
      "member_casual         5500000\n",
      "dtype: int64\n",
      "\n",
      "Total memory usage:\n",
      "57.16 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# create a OS-independent pointer to the csv file created by Setup\n",
    "csv_path = Path('./data/202410-divvy-tripdata-100k.csv')\n",
    "\n",
    "# load and explore the data below (create additional code / markdown cells as necessary)\n",
    "\n",
    "# # Load the data as-is \n",
    "df_raw = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Shape:\", df_raw.shape)\n",
    "\n",
    "# Check data types explicitly ( We can see this in info)\n",
    "print(\"\\n---------dtype---------\")\n",
    "display(df_raw.dtypes)\n",
    "\n",
    "print(\"\\n--- df_raw.info() ---\")\n",
    "df_raw.info()\n",
    "\n",
    "print (\"\\n-----------Quick statistical information--------\")\n",
    "display(df_raw.describe())\n",
    "\n",
    "# Preview first few rows\n",
    "print(\"\\n-------------First 5 rows ----------------\")\n",
    "display(df_raw.head())\n",
    "\n",
    "# Memory usage per column and total (same format as Task 7a)\n",
    "print(\"\\nMemory usage per column:\")\n",
    "print(df_raw.memory_usage(deep=True))\n",
    "\n",
    "print(\"\\nTotal memory usage:\")\n",
    "print(f\"{df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### Task 2b: Reload with Proper Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "1. Repeat step 2a.1 to reload the data. Use the `dtype` and `parse_dates` arguments to properly assign categorical and date types. Assign the result to the variable name `rides`.\n",
    "2. After loading, use `rides.info()` to confirm the type changes.\n",
    "3. Use `memory_usage` to compare the resulting size with that from step 2a.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- rides.info() ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   ride_id             100000 non-null  string        \n",
      " 1   rideable_type       100000 non-null  string        \n",
      " 2   started_at          100000 non-null  datetime64[ns]\n",
      " 3   ended_at            100000 non-null  datetime64[ns]\n",
      " 4   start_station_name  89623 non-null   string        \n",
      " 5   start_station_id    89623 non-null   string        \n",
      " 6   end_station_name    89485 non-null   string        \n",
      " 7   end_station_id      89485 non-null   string        \n",
      " 8   start_lat           100000 non-null  float64       \n",
      " 9   start_lng           100000 non-null  float64       \n",
      " 10  end_lat             99913 non-null   float64       \n",
      " 11  end_lng             99913 non-null   float64       \n",
      " 12  member_casual       100000 non-null  string        \n",
      "dtypes: datetime64[ns](2), float64(4), string(7)\n",
      "memory usage: 9.9 MB\n",
      "\n",
      "Memory usage per column:\n",
      "Index                     132\n",
      "ride_id               6500000\n",
      "rideable_type         6150493\n",
      "started_at             800000\n",
      "ended_at               800000\n",
      "start_station_name    7081717\n",
      "start_station_id      5510377\n",
      "end_station_name      7083099\n",
      "end_station_id        5510515\n",
      "start_lat              800000\n",
      "start_lng              800000\n",
      "end_lat                800000\n",
      "end_lng                800000\n",
      "member_casual         5500000\n",
      "dtype: int64\n",
      "\n",
      "Total memory usage:\n",
      "45.91 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the data path\n",
    "csv_path = Path('./data/202410-divvy-tripdata-100k.csv')\n",
    "\n",
    "# Reload the data with correct dtypes (I included the float columns as well, even though it wasn’t necessary)\n",
    "rides = pd.read_csv(\n",
    "    csv_path,\n",
    "    dtype={\n",
    "        \"ride_id\": \"string\",\n",
    "        \"rideable_type\": \"string\",\n",
    "        \"start_station_name\": \"string\",\n",
    "        \"start_station_id\": \"string\",\n",
    "        \"end_station_name\": \"string\",\n",
    "        \"end_station_id\": \"string\",\n",
    "        \"member_casual\": \"string\",\n",
    "        \"start_lat\": \"float64\",\n",
    "        \"start_lng\": \"float64\",\n",
    "        \"end_lat\": \"float64\",\n",
    "        \"end_lng\": \"float64\"\n",
    "    },\n",
    "    parse_dates=[\"started_at\", \"ended_at\"]\n",
    ")\n",
    "\n",
    "# --- Verify the changes ---\n",
    "print(\"\\n--- rides.info() ---\")\n",
    "rides.info()\n",
    "\n",
    "# --- Memory usage per column and total ---\n",
    "print(\"\\nMemory usage per column:\")\n",
    "print(rides.memory_usage(deep=True))\n",
    "\n",
    "print(\"\\nTotal memory usage:\")\n",
    "print(f\"{rides.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "#### Task 2c: Explore Structure and Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Using the `rides` DataFrame from Task 2b:\n",
    "\n",
    "1. Determine the range of starting dates in the dataframe using the `min` and `max` methods.\n",
    "2. Count the number of missing values in each column. See the section of the same name in lecture 06b.\n",
    "3. Convert the Series from step 2 to a DataFrame using `.to_frame(name='count')`, then add a column called 'percentage' that calculates the percentage of missing values for each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start date range:\n",
      "From 2024-09-30 23:12:01.622000 to 2024-10-31 23:54:02.851000\n",
      "\n",
      "Missing values per column:\n",
      "ride_id                   0\n",
      "rideable_type             0\n",
      "started_at                0\n",
      "ended_at                  0\n",
      "start_station_name    10377\n",
      "start_station_id      10377\n",
      "end_station_name      10515\n",
      "end_station_id        10515\n",
      "start_lat                 0\n",
      "start_lng                 0\n",
      "end_lat                  87\n",
      "end_lng                  87\n",
      "member_casual             0\n",
      "dtype: int64\n",
      "\n",
      "Missing values per column and percentage:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ride_id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rideable_type</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>started_at</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ended_at</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_station_name</th>\n",
       "      <td>10377</td>\n",
       "      <td>10.377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_station_id</th>\n",
       "      <td>10377</td>\n",
       "      <td>10.377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_station_name</th>\n",
       "      <td>10515</td>\n",
       "      <td>10.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_station_id</th>\n",
       "      <td>10515</td>\n",
       "      <td>10.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_lat</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_lng</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_lat</th>\n",
       "      <td>87</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_lng</th>\n",
       "      <td>87</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>member_casual</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count  percentage\n",
       "ride_id                 0       0.000\n",
       "rideable_type           0       0.000\n",
       "started_at              0       0.000\n",
       "ended_at                0       0.000\n",
       "start_station_name  10377      10.377\n",
       "start_station_id    10377      10.377\n",
       "end_station_name    10515      10.515\n",
       "end_station_id      10515      10.515\n",
       "start_lat               0       0.000\n",
       "start_lng               0       0.000\n",
       "end_lat                87       0.087\n",
       "end_lng                87       0.087\n",
       "member_casual           0       0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rides DataFrame from Task 2b\n",
    "\n",
    "# date ranges \n",
    "start_date_min = rides[\"started_at\"].min()\n",
    "start_date_max = rides[\"started_at\"].max()\n",
    "print(f\"Start date range:\\nFrom {start_date_min} to {start_date_max}\\n\")\n",
    "\n",
    "# number of missing values from lecture 06b\n",
    "missing_counts = rides.isna().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Convert the Series to a DataFrame and calculate the percentage ( missing value in each column/total value in each column)\n",
    "missing_df = (missing_counts.to_frame(name='count'))\n",
    "percentage_value = (missing_counts / len(rides)) * 100\n",
    "missing_df['percentage'] = percentage_value\n",
    "print(\"\\nMissing values per column and percentage:\")\n",
    "display(missing_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Task 2d: Create Trip Duration Column and Set Index\n",
    "\n",
    "Before setting the index, create a derived column for trip duration:\n",
    "\n",
    "1. Calculate trip_duration_min by subtracting `started_at` from `ended_at`, then converting to minutes using `.dt.total_seconds() / 60`\n",
    "3. Display basic statistics (min, max, mean) for the new column using `.describe()`\n",
    "4. Show the first few rows with `started_at`, `ended_at`, and `trip_duration_min` to verify the calculation\n",
    "5. Set `started_at` as the DataFrame's index. Verify the change by printing the index and displaying the first few rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip duration statistics (in minutes):\n",
      "count    100000.000000\n",
      "mean         16.144576\n",
      "std          52.922539\n",
      "min           0.006533\n",
      "25%           5.489271\n",
      "50%           9.423592\n",
      "75%          16.407171\n",
      "max        1499.949717\n",
      "Name: trip_duration_min, dtype: float64\n",
      "\n",
      "Sample rows with trip duration:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>trip_duration_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-09-30 23:12:01.622</td>\n",
       "      <td>2024-10-01 00:20:00.674</td>\n",
       "      <td>67.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-30 23:19:25.409</td>\n",
       "      <td>2024-10-01 00:42:09.933</td>\n",
       "      <td>82.742067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-09-30 23:32:24.672</td>\n",
       "      <td>2024-10-01 00:23:18.647</td>\n",
       "      <td>50.899583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-09-30 23:42:11.207</td>\n",
       "      <td>2024-10-01 00:10:16.831</td>\n",
       "      <td>28.093733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-09-30 23:49:25.380</td>\n",
       "      <td>2024-10-01 00:06:27.476</td>\n",
       "      <td>17.034933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               started_at                ended_at  trip_duration_min\n",
       "0 2024-09-30 23:12:01.622 2024-10-01 00:20:00.674          67.984200\n",
       "1 2024-09-30 23:19:25.409 2024-10-01 00:42:09.933          82.742067\n",
       "2 2024-09-30 23:32:24.672 2024-10-01 00:23:18.647          50.899583\n",
       "3 2024-09-30 23:42:11.207 2024-10-01 00:10:16.831          28.093733\n",
       "4 2024-09-30 23:49:25.380 2024-10-01 00:06:27.476          17.034933"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'started_at' as an index verification:\n",
      "started_at\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "      <th>trip_duration_min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>started_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:12:01.622</th>\n",
       "      <td>67BB74BD7667BAB7</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-10-01 00:20:00.674</td>\n",
       "      <td>Oakley Ave &amp; Touhy Ave</td>\n",
       "      <td>bdd4c3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>42.012342</td>\n",
       "      <td>-87.688243</td>\n",
       "      <td>41.970000</td>\n",
       "      <td>-87.650000</td>\n",
       "      <td>casual</td>\n",
       "      <td>67.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:19:25.409</th>\n",
       "      <td>5AF1AC3BA86ED58C</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-10-01 00:42:09.933</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Benson Ave &amp; Church St</td>\n",
       "      <td>a10cf0</td>\n",
       "      <td>42.070000</td>\n",
       "      <td>-87.730000</td>\n",
       "      <td>42.048214</td>\n",
       "      <td>-87.683485</td>\n",
       "      <td>casual</td>\n",
       "      <td>82.742067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:32:24.672</th>\n",
       "      <td>7961DD2FC1280CDC</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-10-01 00:23:18.647</td>\n",
       "      <td>St. Clair St &amp; Erie St</td>\n",
       "      <td>9c619a</td>\n",
       "      <td>LaSalle St &amp; Illinois St</td>\n",
       "      <td>fbd1ad</td>\n",
       "      <td>41.894345</td>\n",
       "      <td>-87.622798</td>\n",
       "      <td>41.890762</td>\n",
       "      <td>-87.631697</td>\n",
       "      <td>member</td>\n",
       "      <td>50.899583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:42:11.207</th>\n",
       "      <td>2E16892DEEF4CC19</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-10-01 00:10:16.831</td>\n",
       "      <td>Ashland Ave &amp; Chicago Ave</td>\n",
       "      <td>72a04d</td>\n",
       "      <td>Loomis St &amp; Archer Ave</td>\n",
       "      <td>896337</td>\n",
       "      <td>41.895954</td>\n",
       "      <td>-87.667728</td>\n",
       "      <td>41.841633</td>\n",
       "      <td>-87.657435</td>\n",
       "      <td>casual</td>\n",
       "      <td>28.093733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:49:25.380</th>\n",
       "      <td>AAF0220F819BEE01</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-10-01 00:06:27.476</td>\n",
       "      <td>900 W Harrison St</td>\n",
       "      <td>11da85</td>\n",
       "      <td>900 W Harrison St</td>\n",
       "      <td>11da85</td>\n",
       "      <td>41.874754</td>\n",
       "      <td>-87.649807</td>\n",
       "      <td>41.874754</td>\n",
       "      <td>-87.649807</td>\n",
       "      <td>member</td>\n",
       "      <td>17.034933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ride_id  rideable_type  \\\n",
       "started_at                                                 \n",
       "2024-09-30 23:12:01.622  67BB74BD7667BAB7  electric_bike   \n",
       "2024-09-30 23:19:25.409  5AF1AC3BA86ED58C  electric_bike   \n",
       "2024-09-30 23:32:24.672  7961DD2FC1280CDC   classic_bike   \n",
       "2024-09-30 23:42:11.207  2E16892DEEF4CC19   classic_bike   \n",
       "2024-09-30 23:49:25.380  AAF0220F819BEE01  electric_bike   \n",
       "\n",
       "                                       ended_at         start_station_name  \\\n",
       "started_at                                                                   \n",
       "2024-09-30 23:12:01.622 2024-10-01 00:20:00.674     Oakley Ave & Touhy Ave   \n",
       "2024-09-30 23:19:25.409 2024-10-01 00:42:09.933                       <NA>   \n",
       "2024-09-30 23:32:24.672 2024-10-01 00:23:18.647     St. Clair St & Erie St   \n",
       "2024-09-30 23:42:11.207 2024-10-01 00:10:16.831  Ashland Ave & Chicago Ave   \n",
       "2024-09-30 23:49:25.380 2024-10-01 00:06:27.476          900 W Harrison St   \n",
       "\n",
       "                        start_station_id          end_station_name  \\\n",
       "started_at                                                           \n",
       "2024-09-30 23:12:01.622           bdd4c3                      <NA>   \n",
       "2024-09-30 23:19:25.409             <NA>    Benson Ave & Church St   \n",
       "2024-09-30 23:32:24.672           9c619a  LaSalle St & Illinois St   \n",
       "2024-09-30 23:42:11.207           72a04d    Loomis St & Archer Ave   \n",
       "2024-09-30 23:49:25.380           11da85         900 W Harrison St   \n",
       "\n",
       "                        end_station_id  start_lat  start_lng    end_lat  \\\n",
       "started_at                                                                \n",
       "2024-09-30 23:12:01.622           <NA>  42.012342 -87.688243  41.970000   \n",
       "2024-09-30 23:19:25.409         a10cf0  42.070000 -87.730000  42.048214   \n",
       "2024-09-30 23:32:24.672         fbd1ad  41.894345 -87.622798  41.890762   \n",
       "2024-09-30 23:42:11.207         896337  41.895954 -87.667728  41.841633   \n",
       "2024-09-30 23:49:25.380         11da85  41.874754 -87.649807  41.874754   \n",
       "\n",
       "                           end_lng member_casual  trip_duration_min  \n",
       "started_at                                                           \n",
       "2024-09-30 23:12:01.622 -87.650000        casual          67.984200  \n",
       "2024-09-30 23:19:25.409 -87.683485        casual          82.742067  \n",
       "2024-09-30 23:32:24.672 -87.631697        member          50.899583  \n",
       "2024-09-30 23:42:11.207 -87.657435        casual          28.093733  \n",
       "2024-09-30 23:49:25.380 -87.649807        member          17.034933  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trip duration in minutes\n",
    "rides['trip_duration_min'] = (rides['ended_at'] - rides['started_at']).dt.total_seconds() / 60\n",
    "\n",
    "# basic statistics for the new column (.describe)\n",
    "print(\"Trip duration statistics (in minutes):\")\n",
    "print(rides['trip_duration_min'].describe())\n",
    "\n",
    "# the first few rows (started_at, ended_at and trip_duration_min)(verification)\n",
    "print(\"\\nSample rows with trip duration:\")\n",
    "display(rides[['started_at', 'ended_at', 'trip_duration_min']].head())\n",
    "\n",
    "#started_at as the DataFrame index and verify\n",
    "rides = rides.set_index('started_at')\n",
    "\n",
    "print(\"\\n'started_at' as an index verification:\")\n",
    "print(rides.index.name)\n",
    "display(rides.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Reflect on problem 2 and answer (briefly / concisely) the following questions:\n",
    "\n",
    "1. What types did Pandas assign to `started_at` and `member_casual` in Task 2a? Why might these defaults be problematic?\n",
    "2. Look at the values in the station ID fields. Based on what you learned about git commit IDs in HW3a, how do you think the station IDs were derived?\n",
    "3. Explain in your own words what method chaining is, what `df.isna().sum()` does and how it works.\n",
    "4. Assume you found ~10% missing values in station columns but ~0% in coordinates. What might explain this? How might you handle the affected rows?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "***\n",
    "#### What types did Pandas assign to started_at and member_casual in Task 2a? Why might these defaults be problematic?\n",
    "Pandas selected or set started_at to an `object type` instead of a `datetime`, and member_casual to an `object` type instead of a `category`. This can be a problem because objects consume more memory and don’t support time-based or categorical operations, such as sorting by date or grouping by user type, efficiently.\n",
    "\n",
    "Note: I explicitly defined started_at as a datetime and member_casual as a string to ensure proper cleaner operations and type handling  \n",
    "\n",
    "#### Look at the values in the station ID fields. Based on what you learned about git commit IDs in HW3a, how do you think the station IDs were derived?  \n",
    "The station ID values seem to be random alphanumeric strings (for instance, \"bdd4c3\"), like git commit IDs. This indicates they were generated using a hashing or encoding method to make each station uniquely identifiable rather than using simple numbers.\n",
    "\n",
    "#### Explain in your own words what method chaining is, what df.isna().sum() does and how it works. \n",
    "Method chaining means applying multiple pandas methods one after another in a single line, where each method’s output becomes the input for the next.\n",
    "`\"df.isna().sum()\"` first checks which values are missing (`isna() `), then counts how many missing values there are per column (`sum()`).\n",
    "\n",
    "#### Assume you found ~10% missing values in station columns but ~0% in coordinates. What might explain this? How might you handle the affected rows?\n",
    "If the station columns contain around 10% missing values and the coordinates columns are almost filled in, then the location data was most likely recorded, but the station names or IDs were not linked properly. I could handle these missing rows by either removing them if they’re few, or filling them using nearby coordinates or matching patterns from similar values.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Compare memory usage results in 2a.3 and 2b.3. What caused the change? Why are these numbers different from what is reported at the bottom of `df.info()`? Which should you use if data size is a concern?\n",
    "\n",
    "Working with DataFrames typically requires 5-10x the dataset size in available RAM. On a system with 16GB, assuming about 30% overhead from the operating system and other programs, what range of dataset sizes would be safely manageable? Calculate using both 5x (optimistic) and 10x (conservative) multipliers, then explain which you'd recommend for reliable work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "***\n",
    "#### Compare memory usage results\n",
    "In Task 2a.3, the DataFrame used the default data types assigned by Pandas (object for datetimes and object for text).On the other hand, in Task 2b.3, I reloaded the dataset using explicit data types, converting text columns to string and parsing dates with datetime64. This change lowered memory usage because string and datetime store data more efficiently than generic object types.\n",
    "\n",
    "#### `df.info` and what should I use\n",
    "The numbers from `df.memory_usage(deep=True)` differ from those shown at the bottom of df.info() because df.info() gives only a rough estimate and, by default, does not include the full memory footprint of Python objects. \n",
    "\n",
    "What should you use?   \n",
    "When memory efficiency matters, the more accurate metric is `df.memory_usage(deep=True).sum()` since it accounts for all underlying data.\n",
    "\n",
    "#### Working with DataFrames\n",
    "A DataFrame normally requires in-memory 5 to 10 times its size for intermediate results and computations. 16 GB of RAM exists in a computer. The operating system and other applications use 30% of the RAM. Python has about 11.2 GB of free memory available for use.  \n",
    "\n",
    "Based on that:   \n",
    "\n",
    "Optimistic (5 ×) = 11.2 GB /5 = 2.24 GB\n",
    "\n",
    "Conservative (10 ×) = 11.2 GB /10 = 1.12 GB\n",
    "\n",
    "Therefore, a dataset between 1 GB to 2 GB is safely manageable on a 16 GB system. For reliable work, I would recommend following the conservative limit to avoid crashes during heavy processing.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Problem 3: Filtering and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "With clean data loaded, you can now filter and transform it to answer specific questions. This problem focuses on Pandas' powerful indexing and filtering capabilities, along with creating derived columns that enable deeper analysis.\n",
    "\n",
    "You'll continue working with the `rides` DataFrame from Problem 2, which has `started_at` set as the index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "#### Task 3a: Boolean Indexing and Membership Testing\n",
    "\n",
    "Use boolean indexing and the `isin()` method to answer these questions:\n",
    "\n",
    "1. How many trips were taken by *members* using *electric bikes*? Use `&` to combine conditions.\n",
    "2. What percentage of all trips does this represent?\n",
    "3. How many trips started at any of these three stations: \"Streeter Dr & Grand Ave\", \"DuSable Lake Shore Dr & Monroe St\", or \"Kingsbury St & Kinzie St\"? Use `isin()`.\n",
    "\n",
    "Note: Remember to use parentheses around each condition when combining with `&`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips by members using electric bikes: 33,121\n",
      "\n",
      "Percentage of total trips: 33.12%\n",
      "\n",
      "Number of trips started at the three stations: 2,702\n"
     ]
    }
   ],
   "source": [
    "# Question 1 (How many trips..) \n",
    "member_electric = rides[(rides['member_casual'] == 'member') & (rides['rideable_type'] == 'electric_bike')]\n",
    "num_member_electric = len(member_electric)\n",
    "print(f\"Number of trips by members using electric bikes: {num_member_electric:,}\")\n",
    "\n",
    "# Question 2 ( What percentage ..)\n",
    "total_trips = len(rides)\n",
    "percent_member_electric = (num_member_electric / total_trips) * 100\n",
    "print(f\"\\nPercentage of total trips: {percent_member_electric:.2f}%\")\n",
    "\n",
    "# Question 3 ( How many trips started at..) \n",
    "stations = [\n",
    "    \"Streeter Dr & Grand Ave\",\n",
    "    \"DuSable Lake Shore Dr & Monroe St\",\n",
    "    \"Kingsbury St & Kinzie St\"\n",
    "]\n",
    "station_trips = rides[rides['start_station_name'].isin(stations)]\n",
    "num_station_trips = len(station_trips)\n",
    "print(f\"\\nNumber of trips started at the three stations: {num_station_trips:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "#### Task 3b: Create Derived Columns from Datetime\n",
    "\n",
    "Add two categorical columns to the rides DataFrame based on trip start time:\n",
    "\n",
    "1. `is_weekend`: Boolean column that is True for Saturday/Sunday trips. Use .dt.dayofweek on the index (Monday=0, Sunday=6).\n",
    "2. `time_of_day`: String categories based on start hour:\n",
    "   - \"Morning Rush\" if hour is 7, 8, or 9\n",
    "   - \"Evening Rush\" if hour is 16, 17, or 18\n",
    "   - \"Midday\" for all other hours\n",
    "\n",
    "For step 2, initialize the column to \"Midday\", then use .loc[mask, 'time_of_day'] with boolean masks to assign rush hour categories. Extract hour using .dt.hour on the index.\n",
    "\n",
    "After creating both columns, use value_counts() on time_of_day to show the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_of_day distribution:\n",
      "time_of_day\n",
      "Midday          55912\n",
      "Evening Rush    28218\n",
      "Morning Rush    15870\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Task 3b \n",
    "# Question 1 ( is_weekend)\n",
    "rides['is_weekend'] = rides.index.dayofweek.isin([5, 6])\n",
    "\n",
    "# Question 2( time_of_day)\n",
    "rides['time_of_day'] = 'Midday'\n",
    "\n",
    "# Extract hours\n",
    "hrs = rides.index.hour\n",
    "\n",
    "# Boolean masks\n",
    "morning_mask = hrs.isin([7, 8, 9])\n",
    "evening_mask = hrs.isin([16, 17, 18])\n",
    "\n",
    "# Assign categories\n",
    "rides.loc[morning_mask, 'time_of_day'] = 'Morning Rush'\n",
    "rides.loc[evening_mask, 'time_of_day'] = 'Evening Rush'\n",
    "\n",
    "# Store as category ( to save memory)\n",
    "rides['time_of_day'] = rides['time_of_day'].astype('category')\n",
    "\n",
    "# Show distribution\n",
    "print(\"time_of_day distribution:\")\n",
    "print(rides['time_of_day'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "#### Task 3c: Complex Filtering with query()\n",
    "\n",
    "Use the `query()` method to find trips that meet **all** of these criteria:\n",
    "- Casual riders (not members)\n",
    "- Weekend trips  \n",
    "- Duration greater than 20 minutes\n",
    "- Electric bikes\n",
    "\n",
    "Report:\n",
    "1. How many trips match these criteria?\n",
    "2. What percentage of all trips do they represent?\n",
    "3. What is the average duration of these trips?\n",
    "\n",
    "Hint: Column names work directly in `query()` strings. Combine conditions with `and`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips matching criteria: 1501\n",
      "Percentage of all trips: 1.50%\n",
      "Average duration (minutes): 40.37\n"
     ]
    }
   ],
   "source": [
    "# Filter rides based on all the given criteria using query()\n",
    "filtered_rides = rides.query(\n",
    "    \"member_casual == 'casual' and is_weekend == True and trip_duration_min > 20 and rideable_type == 'electric_bike'\"\n",
    ")\n",
    "\n",
    "# how many trips match\n",
    "num_trips = len(filtered_rides)\n",
    "\n",
    "# What percentage of all trips do they represent\n",
    "percent_trips = (num_trips / len(rides)) * 100\n",
    "\n",
    "# the average duration of these trips\n",
    "avg_duration = filtered_rides['trip_duration_min'].mean()\n",
    "\n",
    "# Display results\n",
    "print(f\"Number of trips matching criteria: {num_trips}\")\n",
    "print(f\"Percentage of all trips: {percent_trips:.2f}%\")\n",
    "print(f\"Average duration (minutes): {avg_duration:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "#### Task 3d: Explicit Indexing Practice\n",
    "\n",
    "Practice using `loc[]` and `iloc[]` for different selection tasks:\n",
    "\n",
    "1. Use `iloc[]` to select the first 10 trips, showing only `member_casual`, `rideable_type`, and `trip_duration_min` columns\n",
    "2. Use `loc[]` to select trips from October 15-17 (use date strings `'2024-10-15':'2024-10-17'`), showing the same three columns\n",
    "3. Count how many trips occurred during this date range\n",
    "\n",
    "Note: When using `iloc[]`, remember it's position-based (0-indexed). When using `loc[]` with the datetime index, you can slice using date strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>member_casual</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>trip_duration_min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>started_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:12:01.622</th>\n",
       "      <td>casual</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>67.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:19:25.409</th>\n",
       "      <td>casual</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>82.742067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:32:24.672</th>\n",
       "      <td>member</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>50.899583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:42:11.207</th>\n",
       "      <td>casual</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>28.093733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:49:25.380</th>\n",
       "      <td>member</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>17.034933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30 23:49:40.016</th>\n",
       "      <td>member</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>13.009367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01 00:00:53.414</th>\n",
       "      <td>member</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2.598817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01 00:05:44.954</th>\n",
       "      <td>member</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>0.013433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01 00:06:12.035</th>\n",
       "      <td>member</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>10.472933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01 00:10:03.646</th>\n",
       "      <td>member</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>7.825683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        member_casual  rideable_type  trip_duration_min\n",
       "started_at                                                             \n",
       "2024-09-30 23:12:01.622        casual  electric_bike          67.984200\n",
       "2024-09-30 23:19:25.409        casual  electric_bike          82.742067\n",
       "2024-09-30 23:32:24.672        member   classic_bike          50.899583\n",
       "2024-09-30 23:42:11.207        casual   classic_bike          28.093733\n",
       "2024-09-30 23:49:25.380        member  electric_bike          17.034933\n",
       "2024-09-30 23:49:40.016        member  electric_bike          13.009367\n",
       "2024-10-01 00:00:53.414        member   classic_bike           2.598817\n",
       "2024-10-01 00:05:44.954        member  electric_bike           0.013433\n",
       "2024-10-01 00:06:12.035        member  electric_bike          10.472933\n",
       "2024-10-01 00:10:03.646        member   classic_bike           7.825683"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>member_casual</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>trip_duration_min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>started_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-10-15 00:00:12.781</th>\n",
       "      <td>casual</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2.804233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-15 00:01:20.517</th>\n",
       "      <td>member</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>11.330867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-15 00:05:24.811</th>\n",
       "      <td>member</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>1.868850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-15 00:05:52.984</th>\n",
       "      <td>member</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2.705083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-15 00:06:18.819</th>\n",
       "      <td>member</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>1.600867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-17 23:45:48.739</th>\n",
       "      <td>member</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>14.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-17 23:47:35.040</th>\n",
       "      <td>member</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>13.049867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-17 23:55:34.112</th>\n",
       "      <td>member</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>3.795400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-17 23:56:14.464</th>\n",
       "      <td>member</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>11.937217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-17 23:59:38.103</th>\n",
       "      <td>casual</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>5.266433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7235 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        member_casual  rideable_type  trip_duration_min\n",
       "started_at                                                             \n",
       "2024-10-15 00:00:12.781        casual  electric_bike           2.804233\n",
       "2024-10-15 00:01:20.517        member  electric_bike          11.330867\n",
       "2024-10-15 00:05:24.811        member  electric_bike           1.868850\n",
       "2024-10-15 00:05:52.984        member  electric_bike           2.705083\n",
       "2024-10-15 00:06:18.819        member  electric_bike           1.600867\n",
       "...                               ...            ...                ...\n",
       "2024-10-17 23:45:48.739        member  electric_bike          14.703100\n",
       "2024-10-17 23:47:35.040        member  electric_bike          13.049867\n",
       "2024-10-17 23:55:34.112        member   classic_bike           3.795400\n",
       "2024-10-17 23:56:14.464        member  electric_bike          11.937217\n",
       "2024-10-17 23:59:38.103        casual   classic_bike           5.266433\n",
       "\n",
       "[7235 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips between October 15–17: 7235\n"
     ]
    }
   ],
   "source": [
    "# (1) iloc[] to select the first 10 trips\n",
    "display(rides.iloc[:10][['member_casual', 'rideable_type', 'trip_duration_min']])\n",
    "\n",
    "# (2️) loc[] to select trips between October 15–17 \n",
    "oct_trips = rides.loc['2024-10-15':'2024-10-17', ['member_casual', 'rideable_type', 'trip_duration_min']]\n",
    "display(oct_trips)\n",
    "\n",
    "# how many trips occurred during that date range\n",
    "trip_count = len(oct_trips)\n",
    "print(f\"Number of trips between October 15–17: {trip_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "Reflect on this problem and answer (briefly / concisely) the following questions:\n",
    "\n",
    "1. `isin()` advantages: Compare using `isin(['A', 'B', 'C'])` versus `(col == 'A') | (col == 'B') | (col == 'C')`. Beyond readability, what practical advantage does `isin()` provide when filtering for many values (e.g., 20+ stations)?\n",
    "2. Conditional assignment order: In Task 3b, why did we initialize all values to \"Midday\" before assigning rush hour categories? What would go wrong if you assigned categories in a different order, or didn't set a default?\n",
    "3. `query()` vs boolean indexing: The `query()` method in Task 3c could have been written with boolean indexing instead. When would you choose `query()` over boolean indexing? When might boolean indexing be preferable despite being more verbose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "***\n",
    "#### isin() advantages\n",
    "isin() makes things a lot cleaner and simpler, specifically when you’re testing for many values. Instead of writing something long like `(col == 'A') | (col == 'B') | (col == 'C')`, I can just use `col.isin(['A', 'B', 'C'])`. It’s easier to update, read, and maintain later. It’s also faster because Panda handles it internally using efficient set lookups. This is helpful when filtering large datasets with lots of possible categories, like dozens of stations or IDs.In brief, it improves scalability, readability, and performance.\n",
    "\n",
    "#### Conditional assignment order\n",
    "I started by setting everything to \"Midday\" first so that every row has a default value. After that, I used .loc[] to update the rows that fall into \"Morning Rush\" and \"Evening Rush\". If I didn’t set a default first, I’d end up with missing (NaN) values in the rows that don’t match either condition. And if I did it in the wrong order, like putting \"Midday\" last, it would overwrite the rush-hour labels I already set. So doing it this way keeps everything consistent and avoids mistakes.\n",
    "\n",
    "#### query() vs boolean indexing\n",
    "I think `query()` is really nice when you just want to write simple, readable filters; it feels more natural, like writing a sentence. It’s especially good for quick exploration or when you’re combining a few conditions with `and` or `or`. But Boolean indexing gives me more flexibility when I need to use string or datetime functions, or when I’m building masks programmatically. So I’d say query() is great for readability, while boolean indexing is better when I need more control or I’m doing something more complex.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)\n",
    "\n",
    "Pandas supports a variety of indexing paradigms, including bracket notation (`df['col']`), label-based indexing (`loc[]`), and position-based indexing (`iloc[]`). The lecture recommended using bracket notation only for columns, and loc/iloc for everything else. Explain the rationale: why is this approach better than using bracket notation for everything, even though `df[0:5]` technically works for row slicing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "***\n",
    "Bracket notation, like `df['col']`, is fine for column selection but not for row selection, where it gets error-prone and non-intuitive. This is why the lecture recommends only using bracket notation with column labels, and `.loc[]` or `.iloc[]` for everything else.\n",
    "\n",
    "The main reason is that `.loc[]` and `.iloc[]` make your intention clear. `.loc[]` is for label-based indexing (using row or column names), while .iloc[] is for position-based indexing (using integer positions). On the other hand, `df[0:5]` can behave inconsistently depending on the situation, and it’s easy to mix up what it’s actually returning. By sticking with `.loc[]` and .iloc[], the code becomes much more readable, predictable, and less likely to break if the index changes or becomes non-numeric.\n",
    "\n",
    "Ultimately, using .loc[] and .iloc[] clarifies code through reducing ambiguity, especially when working within larger or more complex DataFrames.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Problem 4: Temporal Analysis and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "Time-based patterns are crucial for understanding bike share usage. In this problem, you'll analyze when trips occur, how usage differs between rider types, and export filtered results. You'll use the datetime index you set in Problem 2 and the derived columns from Problems 2-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "#### Task 4a: Identify Temporal Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "Use the datetime index to extract temporal components and identify usage patterns:\n",
    "\n",
    "1. Extract the *hour* from the index and use `value_counts()` to find the most popular hour for trips. Report the peak hour and how many trips occurred during that hour.\n",
    "2. Extract the *day name* from the index and use `value_counts()` to find the busiest day of the week. Report the day and number of trips.\n",
    "3. Sort the results from step 2 to show days in order from Monday to Sunday (not by trip count). Use `sort_index()`.\n",
    "\n",
    "Hint: Use `.dt.hour` and `.dt.day_name()` on the datetime index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular hour for trips: 17:00 with 10574 trips\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "started_at\n",
       "0      1121\n",
       "1       719\n",
       "2       429\n",
       "3       252\n",
       "4       226\n",
       "5       749\n",
       "6      2281\n",
       "7      4776\n",
       "8      6391\n",
       "9      4703\n",
       "10     4498\n",
       "11     5070\n",
       "12     5818\n",
       "13     6131\n",
       "14     6392\n",
       "15     7541\n",
       "16     9705\n",
       "17    10574\n",
       "18     7939\n",
       "19     4885\n",
       "20     3461\n",
       "21     2725\n",
       "22     2107\n",
       "23     1507\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Busiest day of the week: Wednesday with 16513 trips\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "started_at\n",
       "Monday       11531\n",
       "Tuesday      14970\n",
       "Wednesday    16513\n",
       "Thursday     16080\n",
       "Friday       13691\n",
       "Saturday     14427\n",
       "Sunday       12788\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting hours\n",
    "hour_counts = rides.index.hour.value_counts().sort_index()\n",
    "\n",
    "# Finding the most popular hour and number of trips\n",
    "peak_hour = hour_counts.idxmax()\n",
    "peak_trips = hour_counts.max()\n",
    "\n",
    "print(f\"Most popular hour for trips: {peak_hour}:00 with {peak_trips} trips\")\n",
    "display(hour_counts) # Verification\n",
    "\n",
    "# Extracting the day name from the datetime index and count trips per day\n",
    "day_counts = rides.index.day_name().value_counts()\n",
    "\n",
    "# Reorder days from Monday to Sunday\n",
    "ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_counts = day_counts.reindex(ordered_days)\n",
    "\n",
    "# Find the busiest day\n",
    "busiest_day = day_counts.idxmax()\n",
    "busiest_trips = day_counts.max()\n",
    "\n",
    "print(f\"\\nBusiest day of the week: {busiest_day} with {busiest_trips} trips\")\n",
    "display(day_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### Task 4b: Compare Groups with groupby()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "Use `groupby()` (introduced in 07a) to compare trip characteristics across different groups:\n",
    "\n",
    "1. Calculate the average trip duration by rider type (`member_casual`). Which group takes longer trips on average?\n",
    "2. Calculate the average trip duration by bike type (`rideable_type`). Which bike type has the longest average trip?\n",
    "3. Count the number of trips by rider type using `groupby()` with `.size()`. Compare this with using `value_counts()` on the `member_casual` column - do they give the same result?\n",
    "\n",
    "Note: Use single-key groupby only (one column at a time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average trip duration by rider type:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "member_casual\n",
       "casual    23.978046\n",
       "member    11.984493\n",
       "Name: trip_duration_min, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The group that takes longer trips on average: casual\n",
      "\n",
      "Average trip duration by bike type:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rideable_type\n",
       "classic_bike     20.337410\n",
       "electric_bike    12.033618\n",
       "Name: trip_duration_min, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The bike type with the longest average trip: classic_bike\n",
      "\n",
      "Trip counts using groupby():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "member_casual\n",
       "casual    34686\n",
       "member    65314\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip counts using value_counts():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "member_casual\n",
       "member    65314\n",
       "casual    34686\n",
       "Name: count, dtype: Int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results differ — They are the same but order differ.\n"
     ]
    }
   ],
   "source": [
    "# Average trip duration by rider type\n",
    "avg_duration_rider = rides.groupby('member_casual')['trip_duration_min'].mean()\n",
    "print(\"Average trip duration by rider type:\")\n",
    "display(avg_duration_rider)\n",
    "\n",
    "#  Identify which rider group takes longer trips on average\n",
    "longest_group = avg_duration_rider.idxmax()\n",
    "print(f\"\\nThe group that takes longer trips on average: {longest_group}\")\n",
    "\n",
    "# Average trip duration by bike type\n",
    "avg_duration_bike = rides.groupby('rideable_type')['trip_duration_min'].mean()\n",
    "print(\"\\nAverage trip duration by bike type:\")\n",
    "display(avg_duration_bike)\n",
    "\n",
    "# Identify which bike type has the longest average trip\n",
    "longest_bike = avg_duration_bike.idxmax()\n",
    "print(f\"\\nThe bike type with the longest average trip: {longest_bike}\")\n",
    "\n",
    "# Counting number of trips by rider type \n",
    "groupby_counts = rides.groupby('member_casual').size()\n",
    "value_counts = rides['member_casual'].value_counts()\n",
    "\n",
    "print(\"\\nTrip counts using groupby():\")\n",
    "display(groupby_counts)\n",
    "\n",
    "print(\"Trip counts using value_counts():\")\n",
    "display(value_counts)\n",
    "\n",
    "# Compare the two\n",
    "if groupby_counts.equals(value_counts):\n",
    "    print(\"\\nBoth methods give the same result.\")\n",
    "else:\n",
    "    print(\"\\nResults differ — They are the same but order differ.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "#### Task 4c: Filter, Sample, and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "Create a filtered dataset for weekend electric bike trips and export it:\n",
    "\n",
    "The provided code once again uses Path to create an `output` directory and constructs the full file path as `output/weekend_electric_trips.csv`. Use the `output_file` variable when calling `.to_csv()`.\n",
    "\n",
    "1. Filter for trips where `is_weekend == True` and `rideable_type == 'electric_bike'`\n",
    "2. Use `iloc[]` to select the first 1000 trips from this filtered dataset\n",
    "3. Use `reset_index()` to convert the datetime index back to a column (so it's included in the export)\n",
    "4. Export to CSV with filename `weekend_electric_trips.csv`, including only these columns: `started_at`, `ended_at`, `member_casual`, `trip_duration_min`, `time_of_day`\n",
    "5. Use `index=False` to avoid writing the default numeric index to the file\n",
    "\n",
    "After exporting, report how many total weekend electric bike trips existed before sampling to 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total weekend electric bike trips before sampling: 13026\n",
      "\n",
      "Exported 1000 trips to output\\weekend_electric_trips.csv\n"
     ]
    }
   ],
   "source": [
    "# do not modify this setup code\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_file = output_dir / 'weekend_electric_trips.csv'\n",
    "\n",
    "# Filter for weekend electric bike trips\n",
    "weekend_electric = rides[(rides['is_weekend'] == True) & (rides['rideable_type'] == 'electric_bike')]\n",
    "\n",
    "# the total number of weekend electric bike trips before sampling\n",
    "total_trips = len(weekend_electric)\n",
    "print(f\"Total weekend electric bike trips before sampling: {total_trips}\")\n",
    "\n",
    "# Select the first 1000 trips using iloc\n",
    "sampled_trips = weekend_electric.iloc[:1000]\n",
    "\n",
    "# Reset index to include the datetime index as a regular column\n",
    "sampled_trips = sampled_trips.reset_index()\n",
    "\n",
    "# Export selected columns to CSV\n",
    "columns_to_export = ['started_at', 'ended_at', 'member_casual', 'trip_duration_min', 'time_of_day']\n",
    "sampled_trips[columns_to_export].to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nExported {len(sampled_trips)} trips to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "Reflect on this problem and answer the following questions:\n",
    "\n",
    "1. `groupby() conceptual model`: Explain in your own words what `groupby()` does. Use the phrase \"split-apply-combine\" in your explanation and describe what happens at each stage.\n",
    "2. `value_counts()` vs `groupby()`: In Task 4b.3, you compared two approaches for counting trips by rider type. When would you use `value_counts()` versus `groupby().size()`? Is there a situation where only one of them would work?\n",
    "3. Index management for export: In Task 4c, why did we use `reset_index()` before exporting? What would happen if you exported with the datetime index still in place and used `index=False`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "***\n",
    "#### groupby() conceptual mode\n",
    "The groupby function is usually accomplished in three steps, sometimes referred to as split-apply-combine: splitting the data into groups based on the values in one or more column(s), applying a function (for example. mean, count) to each group, and returning a new DataFrame object that contains the results of all the functions. This is used when it is desired to summarize the data, or look at a sub-section without filtering out each data point.\n",
    "\n",
    "#### value_counts() vs groupby()\n",
    "Both of these can count how many records are in each category, but they’re useful in slightly different ways. I’d use value_counts() when I just want a quick count from a single column; it’s simple and fast. But if I need to count across multiple columns or do other calculations at the same time, groupby().size() is the better choice. For example, if I wanted to count trips by both rider type and bike type, groupby() would be the only one that works.\n",
    "\n",
    "#### Index management for export\n",
    "I used reset_index() here before exporting the CSV so that the datetime index (started_at) would get converted back to a regular old column and therefore saved to the outputted CSV. If I hadn't reset the index and used index=False, then those datetime values wouldn't have been saved to the outputted CSV at all. Resetting the index just ensures that all the information, such as the timestamps, is included in the export\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "Compare `CSV` and _pickle_ formats for data storage and retrieval.\n",
    "\n",
    "Pickle is Python's built-in serialization format that saves Python objects exactly as they exist in memory, preserving all data types, structures, and metadata. Unlike CSV (which converts everything to text), pickle is binary (not human readable) and maintains the complete state of your DataFrame. Also, pickle files only work in Python, while CSV is universal. Read more in the [Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html).\n",
    "\n",
    "The code below investigates an interesting pattern: Do riders take longer trips from scenic lakefront stations even during rush hours? This could indicate tourists or recreational riders using these popular locations for leisure trips during typical commute times. The analysis filters for trips over 15 minutes that started from lakefront stations during morning (7-9am) or evening (4-6pm) rush hours, sorted by duration to see the longest trips first.\n",
    "\n",
    "Run the code below, then answer the interpretation questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 498 long rush-hour trips from lakefront stations\n",
      "\n",
      "CSV file size: 70.12 KB\n",
      "Pickle file size: 25.73 KB\n",
      "Size difference: 44.40 KB\n",
      "\n",
      "Load time comparison:\n",
      "CSV:\n",
      "2.14 ms ± 256 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Pickle:\n",
      "431 μs ± 46.3 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Data types from CSV (without parse_dates):\n",
      "started_at             object\n",
      "ended_at               object\n",
      "start_station_name     object\n",
      "end_station_name       object\n",
      "member_casual          object\n",
      "rideable_type          object\n",
      "trip_duration_min     float64\n",
      "dtype: object\n",
      "\n",
      "Data types from Pickle:\n",
      "started_at            datetime64[ns]\n",
      "ended_at              datetime64[ns]\n",
      "start_station_name    string[python]\n",
      "end_station_name      string[python]\n",
      "member_casual         string[python]\n",
      "rideable_type         string[python]\n",
      "trip_duration_min            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# the following lines were commented out since they were run in 4c\n",
    "# from pathlib import Path\n",
    "# output_dir = Path('output')\n",
    "\n",
    "csv_file = output_dir / 'lakefront_rush_trips.csv'\n",
    "pickle_file = output_dir / 'lakefront_rush_trips.pkl'\n",
    "\n",
    "# Filter for interesting pattern: Long trips (>15 min) during rush hours \n",
    "# from lakefront stations, sorted by duration\n",
    "lakefront_rush = (rides\n",
    "\n",
    " .loc[(rides.index.hour.isin([7, 8, 9, 16, 17, 18])) &\n",
    "\n",
    " (rides['start_station_name'].str.contains('Lake Shore|Lakefront',\n",
    "\n",
    " case=False,\n",
    "\n",
    " na=False)) &\n",
    "\n",
    " (rides['trip_duration_min'] > 15)]\n",
    "\n",
    " .sort_values('trip_duration_min', ascending=False)\n",
    "\n",
    " .head(1000)\n",
    "\n",
    " .reset_index()\n",
    "\n",
    " [['started_at', 'ended_at', 'start_station_name', 'end_station_name',\n",
    "\n",
    " 'member_casual', 'rideable_type', 'trip_duration_min']]\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Found {len(lakefront_rush)} long rush-hour trips from lakefront stations\")\n",
    "\n",
    "# Export to both formats\n",
    "lakefront_rush.to_csv(csv_file, index=False)\n",
    "lakefront_rush.to_pickle(pickle_file)\n",
    "\n",
    "# Compare file sizes\n",
    "csv_size = os.path.getsize(csv_file) / 1024  # Convert to KB\n",
    "pickle_size = os.path.getsize(pickle_file) / 1024\n",
    "print(f\"\\nCSV file size: {csv_size:.2f} KB\")\n",
    "print(f\"Pickle file size: {pickle_size:.2f} KB\")\n",
    "print(f\"Size difference: {abs(csv_size - pickle_size):.2f} KB\")\n",
    "\n",
    "# Compare load times\n",
    "print(\"\\nLoad time comparison:\")\n",
    "print(\"CSV:\")\n",
    "%timeit pd.read_csv(csv_file)\n",
    "print(\"\\nPickle:\")\n",
    "%timeit pd.read_pickle(pickle_file)\n",
    "\n",
    "# Check data type preservation\n",
    "# Note: CSV load without parse_dates loses datetime types\n",
    "csv_loaded = pd.read_csv(csv_file)\n",
    "pickle_loaded = pd.read_pickle(pickle_file)\n",
    "\n",
    "print(\"\\nData types from CSV (without parse_dates):\")\n",
    "print(csv_loaded.dtypes)\n",
    "print(\"\\nData types from Pickle:\")\n",
    "print(pickle_loaded.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "After running the code, answer these questions:\n",
    "\n",
    "1. Method chaining: The analysis uses method chaining with a specific formatting pattern:\n",
    "\n",
    "   ```python\n",
    "   result = (df\n",
    "       .method1()\n",
    "       .method2()\n",
    "       .method3()\n",
    "   )\n",
    "   ```\n",
    "\n",
    "   This wraps the entire chain in parentheses, allowing each method to appear on its own line without backslashes. Discuss why this makes formatting more readable, how it makes debugging easier, how it relates to seeing changes in the code with git diff, and what downsides heavy chaining might have.\n",
    "3. Data types: Compare the dtypes from CSV versus pickle. What types were preserved by pickle that were lost in CSV? Why is this preservation significant for subsequent analysis?\n",
    "4. Trade-offs: Given your observations about size, speed, and type preservation, when would you choose pickle over CSV for your work? When would CSV still be the better choice despite pickle's advantages?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "***\n",
    "#### Method chaining\n",
    "The use of method chaining can improve the readability of code. Method chaining makes the program easier to read because every call is a new line in the code. This obviously shows what actions happen on a list. Sorts, filters, and groups are chained.\n",
    "\n",
    "The format lets people debug or version-control the code by using Git, and makes it obvious any change in something that is being done because each operation exists on its own line. This is useful when something does not work because I can comment it out or test a step in the chain without rewriting the code.\n",
    "\n",
    "However, the downside of heavy method chaining is that if the chain gets too long, it becomes harder to debug or inspect intermediate results. It can also make the code less clear and difficult for someone still learning the language to follow along, since all the transformations are packed into one expression. For complex tasks, breaking the process into smaller steps can sometimes be more readable and easier to maintain. For complex tasks, splitting the process into smaller steps can sometimes be more readable and easier to maintain.\n",
    "\n",
    "#### Data types \n",
    "I looked into the CSV and into the pickle files. I noticed that the data types were preserved in the pickle version of the data. The data types were either modified or lost in the CSV version. For example, the `started_at` and `ended_at columns` stayed as proper datetime objects `(datetime64[ns])` in the pickle file, but when loaded from CSV, they became just plain text (object type).\n",
    "\n",
    "The difference matters because using the right datatypes eases analysis much: with the pickle file, I can use datetime methods like .dt.hour and .dt.day_name() right away. If I do use the CSV version of the data, I would have to parse the dates again in turn which is slower and more error-prone.\n",
    "\n",
    "Overall, pickle preserves the structure and meaning of the data, which is really important for any further analysis or calculations.\n",
    "\n",
    "#### Trade-offs\n",
    "Pickle is generally going to be faster to load, going to take up less file space, and going to preserve all of the data types exactly as they were. So if I'm working in pure Python, and I'm going to be saving and reloading data on an iterative basis, I use pickle, particularly if datetime and category are important. \n",
    "\n",
    "However, the CSV format is still the better option when I want to share my data with others and open it in other software, such as Excel or R. CSV is a text-based format and human-readable, while pickle files only work in Python. Even though pickle is more efficient, CSV is much more portable and safer to use for collaboration or long-term storage. In conclusion, pickle is great for performance and accuracy, while CSV is better for compatibility and sharing.\n",
    "\n",
    "#### What did I change from the original code with this fix? In your opinion, why didn't the original code work and why does this one work as intended? \n",
    "In the fixed version, all of the filtering was put into one boolean expression in a single `.loc()` statement rather than multiple `.loc()` statements. In the original code, every `.loc()`  used a mask based on the entire dataset, but was applied to a DataFrame that had already been filtered in the previous step, which would result in a mismatch between the filtered DataFrame and the mask length.\n",
    "\n",
    "The fixed version works because it combines all the conditions into one DataFrame, and the mask remains aligned with the data and allows missing data to be processed without causing an exception by using `na=False` in `str.contains()`.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Address the following questions in a markdown cell:\n",
    "\n",
    "1. NumPy vs Pandas\n",
    "   - What was the biggest conceptual shift moving from NumPy arrays to Pandas DataFrames?\n",
    "   - Which Pandas concept was most challenging: indexing (loc/iloc), missing data, datetime operations, or method chaining? How did you work through it?\n",
    "2. Real Data Experience\n",
    "   - How did working with real CSV data (with missing values, datetime strings, etc.) differ from hw2b's synthetic NumPy arrays?\n",
    "   - Based on this assignment, what makes Pandas well-suited for data analysis compared to pure NumPy?\n",
    "3. Learning & Application\n",
    "   - Which new skill from this assignment will be most useful for your own data work?\n",
    "   - On a scale of 1-10, how prepared do you feel to use Pandas for your own projects? What would increase that score?\n",
    "4. Feedback\n",
    "   - Time spent: ___ hours (breakdown optional)\n",
    "   - Most helpful part of the assignment: ___\n",
    "   - One specific improvement suggestion: ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "***\n",
    "#### Numpy vs Pandas \n",
    "For me, the biggest conceptual change was using labeled data instead of numeric arrays. NumPy involves arrays of numbers and positional indexing, but Pandas uses a data structure associated with labels. Every column in Pandas has its own label along with multiple data types, making it more like working with a database than a matrix.\n",
    "\n",
    "The hardest concept in pandas, for me, really was the indexing, especially making sense of loc and iloc. In the beginning, the concept of when to use label-based vs. position-based selection, but after a time practing with real data and seeing how loc works with datetime indexes, it started to make more sense.\n",
    "\n",
    "#### Real Data Experience\n",
    "Working with real CSV data was different from working with clean NumPy arrays in that the CSV files we received had missing data, strings, and datetime columns that we needed to transform. I also learned that real-world data always needed cleaning, and Pandas made the process easier than NumPy did.\n",
    "\n",
    "Pandas is well-suited for data analysis because it provides tools for handling missing data, grouping, filtering, and time-based analysis, all things which are hard to do in NumPy. It also integrates naturally with file formats like CSV and makes exploring data faster and intuitive.\n",
    "\n",
    "#### Learning & Application\n",
    "The most useful new skill I learned from this assignment is using `groupby()` for data `aggregation` and `comparison`. Before this, I mostly analyzed data column by column, but groupby() showed me how to quickly summarize large datasets and compare different categories, like member types or bike types, in just a few lines of code\n",
    "\n",
    "I’d rate my confidence around 7. I feel pretty comfortable using Pandas for most of my projects now, but I’d improve even more by practicing more advanced topics like merging datasets and using groupby with multiple keys, not only one key at a time.\n",
    "\n",
    "#### Feedback\n",
    "Time spent: 8 hours (including experimenting and debugging)  \n",
    "\n",
    "Most helpful part of the assignment: The section on datetime operations and indexing, which helped me understand how powerful time-based data handling is in Pandas.  \n",
    "\n",
    "One specific improvement suggestion: It would be nice to have a small cheat sheet summarizing common Pandas methods and their arguments to make it easier to reference while coding or on the exam. \n",
    "\n",
    "## Thank you\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15e0f7-5ce9-438d-ac73-e2313754249d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
